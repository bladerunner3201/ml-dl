{"cells":[{"cell_type":"markdown","id":"299eef96","metadata":{"id":"299eef96"},"source":["## 1. 데이터 분리와 모형 생성, 예측, 평가"]},{"cell_type":"markdown","source":["  - **핵심 비유**: 학생의 진짜 실력은 풀어본 문제가 아닌, **처음 보는 문제**로 평가해야 함. 머신러닝 모델의 성능도 마찬가지임.\n","  - **일반화 (Generalization)**: 모델이 처음 보는 데이터에 대해 정확하게 예측하는 능력.\n","  - **과적합 (Overfitting)**: 모델이 학습 데이터의 패턴, 심지어 노이즈까지 전부 **암기**해버리는 현상.\n","      - **결과**: 학습 데이터에 대해서는 거의 100% 성능을 보이지만, 새로운 데이터에 대해서는 성능이 매우 저조함.\n","  - **해결 전략**: 모델의 진짜 **일반화 성능**을 공정하게 평가하기 위해, 전체 데이터를 학습용(Training Set)과 검증용(Test Set)으로 반드시 분리해야 함."],"metadata":{"id":"TYTD4-2tequD"},"id":"TYTD4-2tequD"},{"cell_type":"markdown","source":["## 2. 핵심 원리 파헤치기 (Deep Dive)"],"metadata":{"id":"KyzlQaDZesKL"},"id":"KyzlQaDZesKL"},{"cell_type":"markdown","source":["  - 머신러닝 모델 개발의 **골든 룰**은 데이터를 두 그룹으로 나누어, 모델의 예측 결과와 실제 정답을 비교하는 것임.\n","\n","  - **학습용 데이터셋 (Training Set)**\n","\n","      - **목적**: 모델을 **학습**시키거나 **훈련**시키는 데 사용되는 데이터. (비유: 교과서, 문제집)\n","      - **역할**: 모델이 데이터의 통계적 관계, 패턴, 최적의 의사결정 경계(Decision Boundary)를 찾도록 함.\n","      - **비율**: 일반적으로 전체 데이터의 약 70~80%를 차지함.\n","\n","  - **검증용 데이터셋 (Test Set)**\n","\n","      - **목적**: 학습이 완료된 모델의 최종 **성능**을 평가하기 위한 데이터.\n","      - **규칙**: 모델 학습 과정에서 단 한 번도 사용되지 않은, **완전히 격리된 데이터**여야 함. (비유: 처음 보는 시험 문제)\n","      - **역할**: 모델의 **일반화 성능**, 즉 진짜 실력을 객관적으로 측정하는 척도로 사용됨.\n","\n","  - **데이터 분할 비율과 편향-분산 트레이드오프**\n","\n","      - 정해진 법칙은 없으며, 편향-분산 트레이드오프(Bias-Variance Tradeoff)와 관련 있음.\n","      - 학습 데이터가 너무 적으면 **높은 편향(과소적합)**, 검증 데이터가 너무 적으면 높은 분산(불안정한 평가)의 문제가 발생할 수 있음.\n","      - 경험적으로 **7:3**이나 **8:2** 비율이 널리 사용됨.\n","\n","  - **층화추출법 (Stratified Sampling)**\n","\n","      - **문제 상황**: **불균형 데이터(Imbalanced Data)**(예: 사기 거래 1%, 정상 거래 99%)를 무작위로 추출하면, 검증용 데이터에 소수 클래스가 포함되지 않을 수 있음.\n","      - **해결책**: 원본 데이터의 **클래스 비율**을 학습용과 검증용 데이터셋 모두에 **동일하게 유지**시켜주는 샘플링 기법.\n","      - **중요성**: 분류 문제에서 모델을 공정하게 평가하기 위한 필수적인 기법임."],"metadata":{"id":"ExCD49seevli"},"id":"ExCD49seevli"},{"cell_type":"markdown","source":["## 3. 실전 코드 분석 및 적용 (Code Walkthrough & Application)\n","\n","### 3.1. `random.sample()` 활용하기\n"],"metadata":{"id":"ZrvGksksev4_"},"id":"ZrvGksksev4_"},{"cell_type":"markdown","source":["  - Python의 기본 **`random`** 모듈을 사용하여 데이터의 **인덱스**를 직접 샘플링하는 기초적인 방법"],"metadata":{"id":"-zoEeD0BezEb"},"id":"-zoEeD0BezEb"},{"cell_type":"code","execution_count":1,"id":"fe984129","metadata":{"id":"fe984129","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759366092078,"user_tz":-540,"elapsed":3440,"user":{"displayName":"이정현","userId":"07909459242714302283"}},"outputId":"361b7ef4-59c2-4ceb-f206-ed3c405d4038"},"outputs":[{"output_type":"stream","name":"stdout","text":["추출된 인덱스 개수: 105\n","--- Train Dataset Head ---\n","   sepal_length  sepal_width  petal_length  petal_width species\n","0           5.1          3.5           1.4          0.2  setosa\n","3           4.6          3.1           1.5          0.2  setosa\n","4           5.0          3.6           1.4          0.2  setosa\n","5           5.4          3.9           1.7          0.4  setosa\n","7           5.0          3.4           1.5          0.2  setosa\n","\n","--- Test Dataset Head ---\n","    sepal_length  sepal_width  petal_length  petal_width species\n","1            4.9          3.0           1.4          0.2  setosa\n","2            4.7          3.2           1.3          0.2  setosa\n","6            4.6          3.4           1.4          0.3  setosa\n","13           4.3          3.0           1.1          0.1  setosa\n","16           5.4          3.9           1.3          0.4  setosa\n"]}],"source":["import random\n","import seaborn as sns\n","import pandas as pd\n","\n","# 재현성을 위해 난수 시드 설정 (언제 실행해도 같은 결과를 얻기 위함)\n","random.seed(10)\n","\n","# iris 데이터셋 로드\n","iris = sns.load_dataset(\"iris\")\n","\n","# 150개 데이터 중 70%에 해당하는 105개의 인덱스를 무작위로 비복원 추출\n","inds = random.sample(population=range(150), k=int(150 * 0.7))\n","print(f\"추출된 인덱스 개수: {len(inds)}\")\n","\n","# 추출된 인덱스 리스트를 기반으로 학습용 데이터셋 생성\n","train = iris.loc[inds, :]\n","train.sort_index(inplace=True) # 보기 쉽게 인덱스 기준으로 정렬\n","\n","# 학습용 데이터셋에 포함되지 않은 나머지 데이터로 검증용 데이터셋 생성\n","# ~ 연산자는 NOT을 의미하며, isin()은 포함 여부를 확인합니다.\n","test = iris.loc[~iris.index.isin(train.index)]\n","test.sort_index(inplace=True) # 보기 쉽게 인덱스 기준으로 정렬\n","\n","print(\"--- Train Dataset Head ---\")\n","print(train.head())\n","print(\"\\n--- Test Dataset Head ---\")\n","print(test.head())"]},{"cell_type":"markdown","source":["##### 코드 해설"],"metadata":{"id":"P-hDr-QJS2fu"},"id":"P-hDr-QJS2fu"},{"cell_type":"markdown","source":[],"metadata":{"id":"GDOvvJife155"},"id":"GDOvvJife155"},{"cell_type":"markdown","id":"8a4393b1","metadata":{"id":"8a4393b1"},"source":["### 4.2. `pandas.DataFrame.sample()` 활용하기"]},{"cell_type":"markdown","source":["  - Pandas 데이터프레임의 **`sample()`** 메소드를 활용하는 더 간편한 방법\n","  \n","    **`frac`** 인자에 비율을 지정함"],"metadata":{"id":"KggsWwBge3e7"},"id":"KggsWwBge3e7"},{"cell_type":"code","execution_count":2,"id":"c7535162","metadata":{"id":"c7535162","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759366120306,"user_tz":-540,"elapsed":48,"user":{"displayName":"이정현","userId":"07909459242714302283"}},"outputId":"ecd27ea9-02a2-4ad6-acad-3b817281baaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Train Dataset Head ---\n","     sepal_length  sepal_width  petal_length  petal_width     species\n","14            5.8          4.0           1.2          0.2      setosa\n","98            5.1          2.5           3.0          1.1  versicolor\n","75            6.6          3.0           4.4          1.4  versicolor\n","16            5.4          3.9           1.3          0.4      setosa\n","131           7.9          3.8           6.4          2.0   virginica\n","\n","--- Test Dataset Head ---\n","   sepal_length  sepal_width  petal_length  petal_width species\n","0           5.1          3.5           1.4          0.2  setosa\n","1           4.9          3.0           1.4          0.2  setosa\n","3           4.6          3.1           1.5          0.2  setosa\n","7           5.0          3.4           1.5          0.2  setosa\n","8           4.4          2.9           1.4          0.2  setosa\n"]}],"source":["import seaborn as sns\n","import pandas as pd\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","# frac=0.7로 전체 데이터의 70%를 무작위로 추출\n","# random_state는 재현성을 위한 시드값입니다.\n","train = iris.sample(frac=0.7, random_state=1)\n","\n","# train 데이터의 인덱스를 제외한 나머지로 test 데이터 구성\n","test = iris.loc[~iris.index.isin(train.index)]\n","\n","print(\"--- Train Dataset Head ---\")\n","print(train.head())\n","print(\"\\n--- Test Dataset Head ---\")\n","print(test.head())"]},{"cell_type":"markdown","id":"4f159a51","metadata":{"id":"4f159a51"},"source":["##### 코드 해설"]},{"cell_type":"markdown","source":[],"metadata":{"id":"Xf-Rru5Ye5sM"},"id":"Xf-Rru5Ye5sM"},{"cell_type":"markdown","id":"215c04ca","metadata":{"id":"215c04ca"},"source":["\n","\n","### 4.3. `sklearn.model_selection.train_test_split()` 활용하기 (표준 방법)"]},{"cell_type":"markdown","source":["  - **`scikit-learn`** 라이브러리의 **`train_test_split`** 함수는 데이터 분리를 위한 가장 **보편적이고 표준적인 방법**임."],"metadata":{"id":"k5FlzttDe7eB"},"id":"k5FlzttDe7eB"},{"cell_type":"code","execution_count":3,"id":"2d11d1cd","metadata":{"id":"2d11d1cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759366407601,"user_tz":-540,"elapsed":344,"user":{"displayName":"이정현","userId":"07909459242714302283"}},"outputId":"57c0228e-a4b4-482b-8db7-172a6932fd3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_X shape: (105, 4)\n","test_X shape: (45, 4)\n","train_y shape: (105,)\n","test_y shape: (45,)\n","\n","--- Test y value counts (Random) ---\n","species\n","versicolor    18\n","setosa        14\n","virginica     13\n","Name: count, dtype: int64\n"]}],"source":["import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","# 독립변수(X)와 종속변수(y) 분리\n","X = iris.iloc[:, :-1] # 마지막 'species' 열을 제외한 모든 특성(feature) 열\n","y = iris.iloc[:, -1]  # 마지막 'species' 열 (타겟, target)\n","\n","# 데이터를 학습용과 검증용으로 분리\n","# test_size=0.3 으로 검증용 데이터 비율을 30%로 지정\n","# random_state=1 로 결과를 고정\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n","\n","# 나눠진 데이터셋들의 크기(shape) 확인\n","print(f\"train_X shape: {train_X.shape}\") # 학습용 독립변수\n","print(f\"test_X shape: {test_X.shape}\")   # 검증용 독립변수\n","print(f\"train_y shape: {train_y.shape}\") # 학습용 종속변수\n","print(f\"test_y shape: {test_y.shape}\")   # 검증용 종속변수\n","\n","print(\"\\n--- Test y value counts (Random) ---\")\n","print(test_y.value_counts())"]},{"cell_type":"markdown","id":"8d563bdd","metadata":{"id":"8d563bdd"},"source":["##### 코드 해설"]},{"cell_type":"markdown","source":[],"metadata":{"id":"b1CaIq0efJuH"},"id":"b1CaIq0efJuH"},{"cell_type":"markdown","id":"084830c4","metadata":{"id":"084830c4"},"source":["\n","\n","### 4.4. 층화추출법 적용하기\n"]},{"cell_type":"markdown","source":["  - **`train_test_split`** 함수의 **`stratify`** 옵션을 사용하면 클래스 비율을 유지하며 분할 가능함."],"metadata":{"id":"SG3VjUfEfLCg"},"id":"SG3VjUfEfLCg"},{"cell_type":"code","execution_count":4,"id":"7a5e13d9","metadata":{"id":"7a5e13d9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759366823261,"user_tz":-540,"elapsed":19,"user":{"displayName":"이정현","userId":"07909459242714302283"}},"outputId":"a6f593af-5afb-40aa-b700-97d4cb6d5466"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Test y value counts (Stratified) ---\n","species\n","virginica     15\n","versicolor    15\n","setosa        15\n","Name: count, dtype: int64\n"]}],"source":["import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","iris = sns.load_dataset(\"iris\")\n","X = iris.iloc[:, :-1]\n","y = iris.species # .iloc[:, -1] 과 동일한 표현입니다.\n","\n","# stratify=y 옵션을 추가하여 y(species)의 클래스 비율을 유지하며 분리\n","train_X, test_X, train_y, test_y = train_test_split(X, y,\n","                                                    stratify=y,\n","                                                    test_size=0.3,\n","                                                    random_state=42)\n","\n","print(\"--- Test y value counts (Stratified) ---\")\n","print(test_y.value_counts())"]},{"cell_type":"markdown","id":"082eaba4","metadata":{"id":"082eaba4"},"source":["##### 코드 해설"]},{"cell_type":"markdown","source":[],"metadata":{"id":"Uxg623llfPGo"},"id":"Uxg623llfPGo"},{"cell_type":"markdown","source":["### 5. 모형 생성, 예측, 평가"],"metadata":{"id":"4yk7G8ltfPYb"},"id":"4yk7G8ltfPYb"},{"cell_type":"markdown","source":["  - 분리된 데이터를 사용하여 실제 머신러닝 모델을 만들고 평가하는 전체 과정을 **의사결정나무**와 **인공신경망** 모델을 예시로 살펴봄."],"metadata":{"id":"C2cnoxDpfP_I"},"id":"C2cnoxDpfP_I"},{"cell_type":"code","execution_count":5,"id":"f7f3d7f8","metadata":{"id":"f7f3d7f8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759366938948,"user_tz":-540,"elapsed":962,"user":{"displayName":"이정현","userId":"07909459242714302283"}},"outputId":"c3d8caeb-02c5-4571-860c-4cf4c4dde8eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["모델 학습을 시작합니다...\n","모델 학습이 완료되었습니다.\n","\n","Decision Tree Accuracy: 0.9778\n","MLP Classifier Accuracy: 1.0000\n","\n","--- Decision Tree Confusion Matrix ---\n","Predicted   setosa  versicolor  virginica\n","Actual                                   \n","setosa          15           0          0\n","versicolor       0          15          0\n","virginica        0           1         14\n","\n","--- MLP Classifier Confusion Matrix ---\n","Predicted   setosa  versicolor  virginica\n","Actual                                   \n","setosa          15           0          0\n","versicolor       0          15          0\n","virginica        0           0         15\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}],"source":["import seaborn as sns\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# 1. 데이터 준비 및 분리 (층화추출법 적용)\n","iris = sns.load_dataset(\"iris\")\n","X = iris.iloc[:, :-1]\n","y = iris.iloc[:, -1]\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n","\n","# 2. 모형 생성 및 학습 (fit)\n","# scikit-learn의 모든 모델은 fit() 메소드로 학습합니다.\n","print(\"모델 학습을 시작합니다...\")\n","# 의사결정나무 모델 생성 및 학습\n","dt_model = DecisionTreeClassifier(random_state=1)\n","dt_model.fit(train_X, train_y)\n","\n","# 인공신경망 모델 생성 및 학습\n","mlp_model = MLPClassifier(hidden_layer_sizes=(50,50,20), max_iter=500, random_state=1, activation='relu')\n","mlp_model.fit(train_X, train_y)\n","print(\"모델 학습이 완료되었습니다.\")\n","\n","# 3. 예측 (predict)\n","# 학습된 모델을 사용해 '처음 보는 데이터'인 test_X의 결과를 예측합니다.\n","dt_pred_y = dt_model.predict(test_X)\n","mlp_pred_y = mlp_model.predict(test_X)\n","\n","# 4. 모형 평가 (score 및 crosstab)\n","# score() 함수는 정확도(accuracy)를 반환합니다. (전체 예측 중 맞은 것의 비율)\n","dt_accuracy = dt_model.score(test_X, test_y)\n","mlp_accuracy = mlp_model.score(test_X, test_y)\n","\n","print(f\"\\nDecision Tree Accuracy: {dt_accuracy:.4f}\")\n","print(f\"MLP Classifier Accuracy: {mlp_accuracy:.4f}\")\n","\n","# crosstab으로 실제값(index)과 예측값(columns)을 비교하는 오차 행렬(Confusion Matrix) 생성\n","print(\"\\n--- Decision Tree Confusion Matrix ---\")\n","print(pd.crosstab(test_y, dt_pred_y, rownames=['Actual'], colnames=['Predicted']))\n","\n","print(\"\\n--- MLP Classifier Confusion Matrix ---\")\n","print(pd.crosstab(test_y, mlp_pred_y, rownames=['Actual'], colnames=['Predicted']))"]},{"cell_type":"markdown","source":["##### 코드 해설"],"metadata":{"id":"tBWUNgcfUbTf"},"id":"tBWUNgcfUbTf"},{"cell_type":"markdown","source":[],"metadata":{"id":"YAapAN8GfTwY"},"id":"YAapAN8GfTwY"},{"cell_type":"markdown","id":"1a6a8387","metadata":{"id":"1a6a8387"},"source":["## 5. 핵심 요약 (Key Takeaways)"]},{"cell_type":"markdown","source":["\n","  - **머신러닝 평가의 황금률**: **훈련 데이터로 테스트하지 말 것**. **일반화 성능**을 측정하기 위해 데이터 분리는 필수이며, 이를 어기면 **과적합**된 모델을 배포하는 실수를 범하게 됨.\n","  - **`train_test_split`이 표준인 이유**: 사용이 간편하고 `random_state`로 **재현성** 보장, `stratify`로 **안정성** 확보가 가능함. Scikit-learn 생태계와 호환성이 높아 가장 표준적인 도구임.\n","  - **신뢰할 수 있는 분류 모델을 위해 `stratify`를 기억하세요**: 특히 **클래스 불균형** 데이터에서 `stratify` 옵션은 원본 데이터의 클래스 분포를 유지시켜, 모델 평가의 **신뢰도를 극적으로 높여줌**."],"metadata":{"id":"ukrtoExJfYSe"},"id":"ukrtoExJfYSe"},{"cell_type":"code","source":[],"metadata":{"id":"QsYm0Ltwg2rY"},"id":"QsYm0Ltwg2rY","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"130A_CDb8oSO-UDFvXLTIaGitVU2prTBN","timestamp":1758092436901}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.x"}},"nbformat":4,"nbformat_minor":5}