{"cells":[{"cell_type":"markdown","source":["### 문제 1: wine 데이터를 주성분 분석 후 분류하기\n","\n","  - `wine` 데이터의 13개 화학 성분 변수를 2개의 주성분으로 압축하고, 클래스별로 잘 분리되는지 확인.\n","\n","#### 1\\. 데이터 불러오기 및 전처리"],"metadata":{"id":"P8qhqLUoF-xH"}},{"cell_type":"code","source":["# 필요한 라이브러리를 불러옵니다.\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# UCI 머신러닝 저장소에서 wine 데이터를 불러옵니다.\n","wine_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None) #\n","\n","# 데이터프레임의 컬럼명을 지정합니다.\n","wine_df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n","                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n","                   'Flavanoids', 'Nonflavanoid phenols',\n","                   'Proanthocyanins', 'Color intensity', 'Hue',\n","                   'OD280/0D315 of diluted wines', 'Proline'] #\n","\n","# 데이터의 첫 5개 행을 확인합니다.\n","print(\"--- 원본 데이터 ---\")\n","print(wine_df.head()) #"],"outputs":[],"execution_count":null,"metadata":{"id":"NS5xNkA8F-xI"}},{"cell_type":"code","source":[],"metadata":{"id":"8keXnAdKFtKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **코드 해설**: `Pandas`로 데이터를 로드하고, `train_test_split`으로 데이터를 나눔. \\*\\*`StandardScaler`\\*\\*를 사용해 각 변수의 스케일을 **표준화**했으며, 이는 각 변수가 PCA 계산에 동등한 영향을 미치도록 하기 위한 필수 전처리 과정임.\n","\n","#### 2\\. 주성분 분석(PCA) 수행"],"metadata":{"id":"WNzhC1oOF-xJ"}},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"id":"HTa4QvRuF-xJ"}},{"cell_type":"markdown","source":["- **코드 해설**: `Numpy`의 `cov` 함수로 공분산 행렬을, `linalg.eig` 함수로 **고유값**과 **고유벡터**를 계산함. 고유값은 각 주성분이 설명하는 분산의 크기를 의미하며, 값이 클수록 더 중요한 주성분임.\n","\n","#### 3\\. 설명된 분산 시각화"],"metadata":{"id":"UGUIo_7RF-xJ"}},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"id":"5Yc4G87RF-xJ"}},{"cell_type":"markdown","source":["- **코드 해설**: 그래프는 각 주성분의 분산 설명력을 보여줌. 단 **2개의 주성분만으로도 전체 분산의 약 60% 가까이 설명**할 수 있음을 알 수 있음. 이는 13개 변수를 2개로 줄여도 데이터의 주요 특징이 상당 부분 유지됨을 의미함.\n","\n","#### 4\\. 데이터 변환 및 시각화"],"metadata":{"id":"MAa5ZbkJF-xJ"}},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"id":"FBj0-iGTF-xJ"}},{"cell_type":"markdown","source":["- **코드 해설**: 생성된 2차원 산점도를 보면, 13개 변수로는 시각화가 어려웠던 데이터가 `PC 1`과 `PC 2` 축 위에서 클래스별로 상당히 잘 군집을 이루는 것을 확인할 수 있음. 이는 PCA가 분류에 유용한 특성을 성공적으로 추출했음을 보여줌.\n","\n","-----\n","\n","### 문제 2: 레드와인 데이터를 SVD로 분해 후 재조립하여 분류하기\n","\n","  - `레드와인` 데이터에 SVD를 적용하여 행렬을 분해한 뒤, 다시 곱해 원본 데이터를 복원하고, 이 데이터로 분류 모델을 학습시켜 SVD 과정이 정보를 잘 보존하는지 확인.\n","\n","<!-- end list -->"],"metadata":{"id":"1cyw7f1IF-xK"}},{"cell_type":"code","source":["# 데이터 불러오기\n","redwine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', delimiter=';') #\n","col_names = redwine.columns #\n","print(\"--- 원본 레드와인 데이터 ---\")\n","print(redwine.head()) #\n","\n","# 피처(X)와 타겟(Y) 분리\n","X = redwine.iloc[:, :-1] #\n","Y = redwine.iloc[:, -1] #\n"],"outputs":[],"execution_count":null,"metadata":{"id":"L0KgsfdPF-xK"}},{"cell_type":"code","source":[],"metadata":{"id":"w0q-IppCF2_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **코드 해설**: SVD로 데이터를 분해했다가 모든 특이값을 사용하여 그대로 재조립하면, 원본과 거의 동일한 데이터가 됨. 이 데이터로 MLP 분류기를 학습시킨 결과 약 52%의 훈련 점수를 얻었으며, 이 점수가 차원 축소 성능 비교의 기준점이 됨.\n","\n","-----\n","\n","### 문제 3: SVD 후 특이값 5개만 선택하여 재조립 후 분류하기\n","\n","  - SVD로 분해한 뒤, 중요도가 높은 상위 5개의 특이값만 사용하여 데이터를 재구성.\n","\n","<!-- end list -->"],"metadata":{"id":"EMkVvqXXF-xK"}},{"cell_type":"code","source":["# 데이터 불러오기\n","redwine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', delimiter=';') #\n","col_names = redwine.columns #\n","\n","# 피처(X)와 타겟(Y) 분리\n","X = redwine.iloc[:, :-1] #\n","Y = redwine.iloc[:, -1] #\n"],"outputs":[],"execution_count":null,"metadata":{"id":"71nzANs0F-xK"}},{"cell_type":"code","source":[],"metadata":{"id":"lAdMXEUdF61h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **코드 해설**: SVD로 얻은 구성요소 중 가장 중요한 5개만 사용하여 데이터를 표현함. 이는 원본 11개 변수를 5개의 새로운 변수로 압축한 것과 같음. 흥미롭게도, 차원을 축소했음에도 MLP 모델의 훈련 점수가 이전(약 52%)보다 오히려 상승함. 이는 SVD가 데이터의 불필요한 노이즈를 제거하고 핵심 패턴만 남겨 모델이 더 잘 학습하도록 도왔을 가능성을 시사함."],"metadata":{"id":"KEv39j-kF-xK"}}],"metadata":{"colab":{"provenance":[{"file_id":"1dF7JQwE6r7Zty9OjOzyOwBassIlWTWuL","timestamp":1762146119088}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}